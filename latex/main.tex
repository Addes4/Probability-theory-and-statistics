\documentclass[a4paper,12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[swedish]{babel}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{siunitx}
\usepackage{titlesec}
\usepackage{float}  % Gör så att [H] fungerar
\usepackage{caption}
\usepackage{multido}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{booktabs}

% Konfiguration för listings (MATLAB-kod)
\lstset{
    language=Matlab,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    backgroundcolor=\color{gray!10},
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    frame=single,
    rulecolor=\color{black},
    tabsize=2,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    extendedchars=true,
    inputencoding=utf8,
    literate={ö}{{\"o}}1 {ä}{{\"a}}1 {å}{{\aa}}1
}

\newcommand{\TabRow}[2]{#1 & #2 \\ \hline}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\E}{\operatorname{E}}

\setlength{\parindent}{0pt}            % Sätter indrag (parindent) för nya stycken till 0 → ingen indragning

\makeatletter
\let\origclearpage\clearpage           % Sparar originalkommandot \clearpage i \origclearpage
\makeatother

\newcommand{\NoChapterPageBreaks}{\let\clearpage\relax}       
% Skapar ett nytt kommando som tar bort sidbrytningar vid kapitel
% (ersätter \clearpage med tomt kommando \relax)

\newcommand{\RestoreChapterPageBreaks}{\let\clearpage\origclearpage}
% Skapar ett nytt kommando som återställer originalbeteendet för sidbrytningar vid kapitel

\geometry{margin=1in}                  % Sätter sidmarginaler till 1 tum runt hela sidan
\raggedbottom                          % Hindrar LaTeX från att stretcha texten vertikalt för att fylla sidan

\addto\captionsenglish{\renewcommand{\chaptername}{}}

\titleformat{\chapter}[block]
  {\normalfont\huge\bfseries}
  {\thechapter\quad}
  {0pt}
  {}
\titlespacing*{\chapter}{0pt}{-20pt}{20pt}


\begin{document}

\begin{titlepage}
    \centering
    \vspace*{9cm}  % trycker ner titeln mot mitten
    
    {\huge\bfseries Sannstat laboration\par}
    \vspace*{1cm}
\end{titlepage}



\newpage


\chapter*{Problem 1: ML- och MK-skattning för Rayleigh-fördelning}

Låt $X$ vara en stokastisk variabel med täthetsfunktion
\begin{equation}
f_X(x) = \begin{cases}
\frac{x}{b^2} e^{-\frac{x^2}{2b^2}} & \text{för } x \geq 0 \\
0 & \text{för } x < 0
\end{cases}
\end{equation}
där $b > 0$ är en parameter. Givet ett stickprov $x_1, x_2, \ldots, x_n$ ska vi bestämma ML-skattningen och MK-skattningen (momentmetoden) av parametern $b$.

\subsection*{ML-skattning (Maximum Likelihood)}

Likelihood-funktionen för stickprovet är
\begin{equation}
L(b) = \prod_{i=1}^n f_X(x_i) = \prod_{i=1}^n \frac{x_i}{b^2} e^{-\frac{x_i^2}{2b^2}} = \frac{\prod_{i=1}^n x_i}{b^{2n}} e^{-\frac{1}{2b^2}\sum_{i=1}^n x_i^2}
\end{equation}

Log-likelihood-funktionen blir
\begin{equation}
\ell(b) = \ln L(b) = \sum_{i=1}^n \ln x_i - 2n \ln b - \frac{1}{2b^2}\sum_{i=1}^n x_i^2
\end{equation}

För att hitta maximum sätter vi derivatan lika med noll:
\begin{equation}
\frac{d\ell}{db} = -\frac{2n}{b} + \frac{1}{b^3}\sum_{i=1}^n x_i^2 = 0
\end{equation}

Multiplicera båda sidor med $b^3$:
\begin{equation}
-2nb^2 + \sum_{i=1}^n x_i^2 = 0
\end{equation}

Lös ut $b^2$:
\begin{equation}
b^2 = \frac{1}{2n}\sum_{i=1}^n x_i^2 = \frac{1}{2}\cdot\frac{1}{n}\sum_{i=1}^n x_i^2
\end{equation}

Därför är ML-skattningen
\begin{equation}
\hat{b}_{\text{ML}} = \sqrt{\frac{1}{2n}\sum_{i=1}^n x_i^2} = \sqrt{\frac{\bar{x^2}}{2}}
\end{equation}
där $\bar{x^2} = \frac{1}{n}\sum_{i=1}^n x_i^2$ är stickprovsmomentet av ordning 2.

\subsection*{MK-skattning}

För momentmetoden sätter vi stickprovsmomentet lika med teoretiskt moment. Först behöver vi beräkna väntevärdet för Rayleigh-fördelningen.

För Rayleigh-fördelning med täthetsfunktion $f_X(x) = \frac{x}{b^2} e^{-\frac{x^2}{2b^2}}$ för $x \geq 0$ är väntevärdet:
\begin{equation}
E[X] = \int_0^{\infty} x \cdot \frac{x}{b^2} e^{-\frac{x^2}{2b^2}} dx
\end{equation}

Genom substitution $u = \frac{x^2}{2b^2}$ kan detta beräknas (använder gamma-funktionen), vilket ger det kända resultatet:
\begin{equation}
E[X] = b\sqrt{\frac{\pi}{2}}
\end{equation}

För momentmetoden sätter vi stickprovsmedelvärdet lika med teoretiskt väntevärde:
\begin{equation}
\bar{x} = E[X] = b\sqrt{\frac{\pi}{2}}
\end{equation}

För momentmetoden sätter vi:
\begin{equation}
\bar{x} = E[X] = b\sqrt{\frac{\pi}{2}}
\end{equation}

där $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$ är stickprovsmedelvärdet. Lös ut $b$:
\begin{equation}
\hat{b}_{\text{MK}} = \frac{\bar{x}}{\sqrt{\pi/2}} = \sqrt{\frac{2}{\pi}} \cdot \bar{x}
\end{equation}

\chapter*{Problem 2: Approximativt konfidensintervall för parametern $b$}

För att härleda ett approximativt konfidensintervall för parametern $b$ använder vi MK-skattningen från Problem 1, $\hat{b}_{\text{MK}} = \sqrt{\frac{2}{\pi}} \cdot \bar{X}$, tillsammans med centrala gränsvärdessatsen. MK-skattningen väljs eftersom den är en linjär funktion av $\bar{X}$ och har enklare asymptotiska egenskaper än ML-skattningen.

\subsection*{Variansberäkning}

För Rayleigh-fördelningen med parameter $b$ har vi $\E[X] = b\sqrt{\pi/2}$. För att beräkna variansen behöver vi $\E[X^2]$. Genom substitution $u = x^2/(2b^2)$ får vi:
\begin{equation}
\E[X^2] = \int_0^{\infty} \frac{x^3}{b^2} e^{-\frac{x^2}{2b^2}} dx = 2b^2 \int_0^{\infty} u e^{-u} du = 2b^2 \Gamma(2) = 2b^2
\end{equation}

Därför är variansen:
\begin{equation}
\Var(X) = \E[X^2] - (\E[X])^2 = 2b^2 - \frac{\pi b^2}{2} = \frac{b^2(4-\pi)}{2}
\end{equation}

\subsection*{Asymptotisk fördelning och konfidensintervall}

Enligt centrala gränsvärdessatsen är $\bar{X}$ approximativt normalfördelat för stora $n$. Eftersom $\hat{b}_{\text{MK}} = \sqrt{2/\pi} \cdot \bar{X}$ är en linjär transformation, följer det att:
\begin{equation}
\hat{b}_{\text{MK}} \sim N\left(b, \frac{b^2(4-\pi)}{\pi n}\right)
\end{equation}
asymptotiskt.

För att konstruera konfidensintervallet standardiserar vi och ersätter den okända parametern $b$ med skattningen $\hat{b}_{\text{MK}}$ i variansen:
\begin{equation}
Z = \frac{\hat{b}_{\text{MK}} - b}{\sqrt{\frac{\hat{b}_{\text{MK}}^2(4-\pi)}{\pi n}}} \approx N(0,1)
\end{equation}

För ett $(1-\alpha)$-konfidensintervall har vi $P(-z_{\alpha/2} \leq Z \leq z_{\alpha/2}) = 1-\alpha$, där $z_{\alpha/2}$ är $(1-\alpha/2)$-kvantilen för standardnormalfördelningen. Genom att lösa ut $b$ får vi det approximativa konfidensintervallet:
\begin{equation}
\left[\hat{b}_{\text{MK}} - z_{\alpha/2}\hat{b}_{\text{MK}}\sqrt{\frac{4-\pi}{\pi n}}, \quad \hat{b}_{\text{MK}} + z_{\alpha/2}\hat{b}_{\text{MK}}\sqrt{\frac{4-\pi}{\pi n}}\right]
\end{equation}

där $z_{\alpha/2}\hat{b}_{\text{MK}}\sqrt{(4-\pi)/(\pi n)}$ är marginalfelet.

\subsection*{Motivering}

Approximationen är rimlig eftersom: (1) MK-skattningen är linjär i $\bar{X}$, vilket gör asymptotiska beräkningar enklare, (2) centrala gränsvärdessatsen ger normalfördelning för stora stickprov, (3) MK-skattningen är konsistent och har enkla slutna formler för momenten.

\chapter*{Problem 3: Linjär regression}

\subsection*{Idén bakom linjär regression}

Linjär regression är en metod för att modellera sambandet mellan en beroende variabel $y$ och en eller flera oberoende variabler $x$ genom en linjär funktion. Grundidén är att hitta den linje (eller hyperplan i flerdimensionella fall) som bäst passar data genom att minimera summan av kvadrerade fel.

För enkel linjär regression har vi modellen:
\begin{equation}
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \quad i = 1, 2, \ldots, n
\end{equation}
där $\beta_0$ är intercept, $\beta_1$ är lutningen, och $\varepsilon_i$ är feltermer som antas vara oberoende och normalfördelade med väntevärde 0 och varians $\sigma^2$.

Minsta kvadratmetoden (OLS, Ordinary Least Squares) minimerar objektfunktionen:
\begin{equation}
Q(\beta_0, \beta_1) = \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2
\end{equation}

Genom att sätta partialderivatorna lika med noll får vi normalekvationerna, vilka kan lösas analytiskt för att ge skattningarna $\hat{\beta_0}$ och $\hat{\beta_1}$.

I vektorform kan modellen skrivas som:
\begin{equation}
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
\end{equation}
där $\mathbf{X}$ är designmatrisen, $\boldsymbol{\beta}$ är parametervektorn, och lösningen blir:
\begin{equation}
\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
\end{equation}

\subsection*{Funktionen \texttt{tools.regress}}

Filen \texttt{tools.py} innehåller funktionen \texttt{regress} som implementerar multipel linjär regression med minsta kvadratmetoden. Funktionen löser modellen:

\begin{equation}
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
\end{equation}

där $\mathbf{y}$ är en vektor av observerade värden med form $(n,)$, $\mathbf{X}$ är en matris av regressorer med form $(n, p)$, $\boldsymbol{\beta}$ är parametervektorn med form $(p,)$, och $\boldsymbol{\varepsilon}$ är en vektor av slumpmässiga fel med form $(n,)$.

\subsection*{Funktionens egenskaper}

Funktionen \texttt{regress} har följande viktiga egenskaper:

\begin{enumerate}
\item \textbf{QR-faktorisering:} Funktionen använder QR-faktorisering (\texttt{np.linalg.qr}) för att lösa normalekvationerna numeriskt stabilt, istället för att direkt invertera $\mathbf{X}^T\mathbf{X}$.

\item \textbf{NaN-hantering:} Alla rader som innehåller NaN-värden i antingen $\mathbf{X}$ eller $\mathbf{y}$ filtreras bort innan beräkningarna.

\item \textbf{Konfidensintervall:} Funktionen beräknar konfidensintervall för varje parameter baserat på t-fördelningen med $n-p$ frihetsgrader, där $n$ är antalet observationer och $p$ är antalet parametrar.

\item \textbf{Residualvarians:} Variansen för residualerna beräknas som:
\begin{equation}
\hat{\sigma}^2 = \frac{\text{SSE}}{n-p} = \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n-p}
\end{equation}
där SSE är summan av kvadrerade residualer.

\item \textbf{Standardfel:} Standardfelet för varje parameter beräknas från diagonalen av $(\mathbf{R}^T\mathbf{R})^{-1}$, där $\mathbf{R}$ kommer från QR-faktoriseringen.
\end{enumerate}

\subsection*{Användning för modellen $w_k = \log(y_k) = \beta_0 + \beta_1 x_k + \varepsilon_k$}

För modellen
\begin{equation}
w_k = \log(y_k) = \beta_0 + \beta_1 x_k + \varepsilon_k \tag{1}
\end{equation}
behöver man:

\begin{enumerate}
\item \textbf{Transformera data:} Beräkna $w_k = \log(y_k)$ för alla observationer.
\item \textbf{Skapa designmatris:} Konstruera designmatrisen $\mathbf{X}$ med en kolumn av ettor (för intercept $\beta_0$) och en kolumn med $x$-värden (för lutning $\beta_1$). Detta ger $\mathbf{X}$ formen:
\begin{equation}
\mathbf{X} = \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n \end{bmatrix}
\end{equation}
\item \textbf{Anropa funktionen:} Funktionen \texttt{regress(X, w, alpha=0.05)} returnerar:
   \begin{itemize}
   \item \texttt{beta}: Vektor med skattade parametrar $[\hat{\beta_0}, \hat{\beta_1}]^T$
   \item \texttt{beta\_int}: Matris med konfidensintervall, där varje rad motsvarar en parameter och kolumnerna är [nedre gräns, övre gräns]
   \end{itemize}
\end{enumerate}

\subsection*{Konfidensintervall}

Konfidensintervallen beräknas enligt:
\begin{equation}
\hat{\beta}_j \pm t_{\alpha/2, n-p} \cdot \text{SE}(\hat{\beta}_j)
\end{equation}
där $t_{\alpha/2, n-p}$ är $(1-\alpha/2)$-kvantilen för t-fördelningen med $n-p$ frihetsgrader, och $\text{SE}(\hat{\beta}_j)$ är standardfelet för parameter $j$. För enkel linjär regression ($p=2$) blir antalet frihetsgrader $n-2$, vilket är korrekt när feltermerna är normalfördelade.


\chapter*{Analys av kod och resultat}

I detta kapitel analyseras implementationen och resultaten från de fem Python-filerna: \texttt{Problem1.py}, \texttt{Problem2.py}, \texttt{Problem3.py}, \texttt{Problem4.py} och \texttt{Problem5.py}. Varje problem implementerar de teoretiska koncepten som beskrivs i förberedelseuppgifterna ovan.

\section*{Problem1.py: Simulering av konfidensintervall}

\subsection*{Syfte och metod}

\texttt{Problem1.py} demonstrerar konceptet med konfidensintervall genom att simulera 100 konfidensintervall för medelvärdet av en normalfördelad variabel. Koden visar visuellt hur många av dessa intervall som innehåller det sanna värdet $\mu = 2$.

\subsection*{Implementation}

Koden simulerar $M = 100$ stickprov, varje med $N = 25$ observationer från en normalfördelning med $\mu = 2$ och $\sigma = 1$. För varje stickprov beräknas:

\begin{itemize}
\item Stickprovsmedelvärdet: $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$
\item 95\% konfidensintervall: $\left[\bar{x} - z_{0.025}\frac{\sigma}{\sqrt{n}}, \bar{x} + z_{0.025}\frac{\sigma}{\sqrt{n}}\right]$
\end{itemize}

där $z_{0.025} \approx 1.96$ är 97.5\%-kvantilen för standardnormalfördelningen.

\subsection*{Visualisering}

Koden skapar en vertikal plot där:
\begin{itemize}
\item Varje horisontell linje representerar ett konfidensintervall
\item Blå linjer indikerar intervall som innehåller det sanna värdet $\mu = 2$
\item Röda linjer indikerar intervall som \emph{inte} innehåller det sanna värdet
\item En grön vertikal linje markerar det sanna värdet $\mu = 2$
\end{itemize}

\subsection*{Resultat och tolkning}

Med 95\% konfidensintervall förväntar vi oss att ungefär 95 av 100 intervall ska innehålla det sanna värdet. I en typisk körning kommer cirka 3-7 intervall (5\% av 100) att missa det sanna värdet, vilket illustreras av de röda linjerna i plotten.

Detta demonstrerar viktiga koncept:
\begin{enumerate}
\item \textbf{Konfidensgrad:} 95\% betyder inte att 95\% av tiden är parametern i intervallet, utan att om vi upprepar experimentet många gånger, kommer 95\% av intervallerna att innehålla det sanna värdet.
\item \textbf{Slumpmässighet:} Varje ny simulering ger ett annat mönster av röda/blå linjer, men i genomsnitt kommer 95\% att vara blå.
\item \textbf{Visuell förståelse:} Plotten gör det lätt att se hur konfidensintervall fungerar i praktiken.
\end{enumerate}

\section*{Problem2.py: ML- och MK-skattning för Rayleigh-fördelning}

\subsection*{Syfte och metod}

\texttt{Problem2.py} implementerar och jämför Maximum Likelihood (ML) och Momentmetoden (MK) skattningar för parametern $b$ i en Rayleigh-fördelning. Koden simulerar $M = 100,000$ observationer från en Rayleigh-fördelning med $b = 4$ och beräknar båda skattningarna.

\subsection*{Implementation}

Koden använder:
\begin{itemize}
\item \texttt{stats.rayleigh.rvs(scale=B, size=M)} för att simulera data med parameter $b = 4$
\item ML-skattning: $\hat{b}_{\text{ML}} = \sqrt{\frac{1}{2n}\sum_{i=1}^n x_i^2}$ (implementerad som \texttt{np.sqrt(np.mean(x**2) / 2)})
\item MK-skattning: $\hat{b}_{\text{MK}} = \sqrt{\frac{2}{\pi}} \cdot \bar{x}$ (implementerad som \texttt{np.sqrt(2/np.pi) * np.mean(x)})
\end{itemize}

\subsection*{Visualisering}

Koden skapar två plotter:
\begin{enumerate}
\item \textbf{Histogram med skattningar:} Visar histogrammet av de simulerade datan tillsammans med:
   \begin{itemize}
   \item Röd stjärna: ML-skattningen
   \item Grön stjärna: MK-skattningen
   \item Blå cirkel: Det sanna värdet $b = 4$
   \end{itemize}
\item \textbf{Histogram med täthetsfunktion:} Visar histogrammet tillsammans med den teoretiska täthetsfunktionen $f_X(x)$ beräknad med ML-skattningen som parameter.
\end{enumerate}

\subsection*{Resultat och tolkning}

Med $M = 100,000$ observationer kommer båda skattningarna att vara mycket nära det sanna värdet $b = 4$:
\begin{itemize}
\item \textbf{ML-skattningen} är asymptotiskt effektiv och bör ge en något bättre skattning för stora stickprov.
\item \textbf{MK-skattningen} är enklare att beräkna och har goda asymptotiska egenskaper.
\item Båda skattningarna är konsistenta, vilket innebär att de konvergerar mot det sanna värdet när stickprovsstorleken ökar.
\end{itemize}

Den andra plotten visar hur väl den skattade täthetsfunktionen passar mot den empiriska fördelningen (histogrammet), vilket är ett sätt att visuellt validera skattningen.

\section*{Problem3.py: Konfidensintervall för Rayleigh-fördelning}

\subsection*{Syfte och metod}

\texttt{Problem3.py} implementerar det approximativa konfidensintervallet för parametern $b$ i en Rayleigh-fördelning, som beskrivs teoretiskt i Problem 2. Koden använder verklig data från \texttt{wave\_data.dat} och beräknar ett 95\% konfidensintervall för $b$.

\subsection*{Implementation}

Koden följer följande steg:
\begin{enumerate}
\item Laddar data från \texttt{Data\_and\_tools/wave\_data.dat}
\item Visualiserar en del av signalen och histogrammet
\item Beräknar ML-skattningen för att plotta täthetsfunktionen: $\hat{b}_{\text{ML}} = \sqrt{\frac{1}{2n}\sum_{i=1}^n y_i^2}$
\item Beräknar MK-skattningen för konfidensintervallet: $\hat{b}_{\text{MK}} = \sqrt{\frac{2}{\pi}} \cdot \bar{y}$
\item Beräknar 95\% konfidensintervall:
   \begin{equation}
   \left[\hat{b}_{\text{MK}} - z_{0.025}\hat{b}_{\text{MK}}\sqrt{\frac{4-\pi}{\pi n}}, \quad \hat{b}_{\text{MK}} + z_{0.025}\hat{b}_{\text{MK}}\sqrt{\frac{4-\pi}{\pi n}}\right]
   \end{equation}
\end{enumerate}

\subsection*{Visualisering}

Koden skapar två plotter:
\begin{enumerate}
\item \textbf{Tidsdomän och histogram:} Övre subplot visar de första 100 datapunkterna (tidsdomän), nedre subplot visar histogrammet av alla data.
\item \textbf{Histogram med konfidensintervall:} Visar histogrammet tillsammans med:
   \begin{itemize}
   \item Grön stjärna (vänster): Nedre gräns för konfidensintervallet
   \item Grön stjärna (höger): Övre gräns för konfidensintervallet
   \item Röd kurva: Teoretisk täthetsfunktion med ML-skattningen
   \end{itemize}
\end{enumerate}

\subsection*{Resultat och tolkning}

Konfidensintervallet ger ett intervall där vi med 95\% konfidens kan säga att den sanna parametern $b$ ligger. Detta är användbart för att:
\begin{itemize}
\item \textbf{Kvantifiera osäkerhet:} Visar inte bara punktskattningen utan också osäkerheten i skattningen.
\item \textbf{Jämföra med teori:} Om vi vet det sanna värdet (t.ex. från simulering), kan vi kontrollera om det ligger inom intervallet.
\item \textbf{Praktisk tillämpning:} För verklig data ger intervallet en uppfattning om hur pålitlig skattningen är.
\end{itemize}

Notera att koden använder MK-skattningen för konfidensintervallet (eftersom den har enklare asymptotiska egenskaper) men ML-skattningen för att plotta täthetsfunktionen (eftersom den ger en bättre visuell anpassning).


\chapter*{Analys av problem4.py}

I detta kapitel analyseras data från \texttt{birth.dat} som innehåller information om 747 första gångers mödrar i Malmö under perioden 1991-1993. Analysen fokuserar på att undersöka fördelningarna för olika variabler samt jämföra födelsevikter mellan rökare och icke-rökare.

\section*{Fördelningar för olika variabler}

\subsection*{Barnets födelsevikt}

Histogrammet för barnets födelsevikt visar en approximativt klokformad (normalliknande) fördelning med en lätt vänsterskewning. Fördelningen har sin topp mellan 3000 och 3500 gram, vilket är typiskt för normala födelsevikter. Den maximala tätheten är strax under 0.0008. 

Fördelningen avtar gradvis mot både lägre och högre vikter. Mycket få barn föds under 1000 gram (extremt låg födelsevikt) eller över 4500 gram. Detta indikerar att majoriteten av barnen i studien har normala födelsevikter, medan extrema värden är ovanliga.

\subsection*{Moderns ålder}

Fördelningen för moderns ålder visar en tydlig högerskewning. Toppen av fördelningen ligger mellan 25 och 28 år, med en maximal täthet strax över 0.12. Detta indikerar att de flesta mödrarna i studien är i denna åldersgrupp.

Fördelningen avtar snabbt för åldrar över 30 år, med en lång svans som sträcker sig mot 40 år. Mycket få mödrar är under 20 år gamla. Denna fördelning är typisk för första gångers mödrar, där de flesta befinner sig i den reproduktiva åldern.

\subsection*{Moderns längd}

Histogrammet för moderns längd visar en något oregelbunden men generellt klokformad fördelning. Det finns två tydliga toppar: en runt 160-162 cm och en annan runt 168-170 cm, med den högsta tätheten strax över 0.07. 

Fördelningen är relativt symmetrisk kring intervallet 160-170 cm, med tätheten som avtar mot extremvärdena. Denna bimodala struktur kan tyda på att det finns två distinkta grupper i populationen, möjligen relaterade till etnisk bakgrund eller andra demografiska faktorer.

\subsection*{Moderns vikt}

Fördelningen för moderns vikt är högerskewad. Toppen ligger runt 60 kg med en maximal täthet strax under 0.07. Tätheten avtar gradvis för högre vikter, vilket bildar en svans som sträcker sig bortom 100 kg. Mycket få mödrar väger mindre än 45 kg.

Denna högerskewning är förväntad för viktfördelningar, eftersom det finns en naturlig nedre gräns men ingen strikt övre gräns. Majoriteten av mödrarna har vikter i intervallet 50-80 kg.

\section*{Jämförelse mellan rökare och icke-rökare}

\subsection*{Låddiagram}

Låddiagrammen visar en tydlig skillnad mellan de två grupperna:

\begin{itemize}
\item \textbf{Icke-rökare:} Medianvärdet och kvartilerna ligger högre än för rökare. Boxen (interkvartilintervallet) är placerad högre på skalan, vilket indikerar att barn till icke-rökare generellt har högre födelsevikter.
\item \textbf{Rökare:} Medianvärdet och kvartilerna ligger lägre. Boxen är placerad på en lägre nivå, vilket visar att barn till rökare har lägre födelsevikter i genomsnitt.
\end{itemize}

Skillnaden mellan medianvärdena är tydlig och statistiskt signifikant, vilket bekräftar det välkända sambandet mellan rökning under graviditeten och lägre födelsevikt.

\subsection*{Kärnestimatorer (KDE)}

Kärnestimatorerna ger en mer detaljerad bild av skillnaderna mellan grupperna:

\begin{itemize}
\item \textbf{Icke-rökare (blå kurva):} Fördelningen är centrerad kring högre vikter, med en topp som ligger högre än för rökare. Kurvan är relativt symmetrisk och täcker ett bredare intervall av vikter.
\item \textbf{Rökare (röd kurva):} Fördelningen är skiftad åt vänster (mot lägre vikter) jämfört med icke-rökare. Toppen ligger på en lägre viktnivå, vilket bekräftar att rökning är associerad med lägre födelsevikter.
\end{itemize}

Kurvorna visar också att fördelningen för rökare kan ha en något annan form, vilket kan tyda på att rökning inte bara skiftar fördelningen utan också kan påverka dess form.

\subsection*{Jämförelse mellan drickare och icke-drickare}

Koden innehåller också en extra analys som jämför födelsevikter mellan kvinnor som dricker alkohol under graviditeten och de som inte dricker eller slutade när de blev gravida. Resultaten visar:

\begin{itemize}
\item \textbf{Non-drinkers:} 540 observationer
\item \textbf{Drinkers:} 167 observationer
\end{itemize}

Låddiagrammen och kärnestimatorerna visar en liknande men mindre uttalad skillnad jämfört med rökning:
\begin{itemize}
\item \textbf{Non-drinkers (blå kurva):} Fördelningen är något skiftad mot högre vikter jämfört med drinkers.
\item \textbf{Drinkers (röd kurva):} Fördelningen är något skiftad mot lägre vikter, men skillnaden är mindre markant än för rökning.
\end{itemize}

Detta indikerar att alkoholkonsumtion kan ha en effekt på födelsevikt, men effekten verkar vara mindre än för rökning. Det är viktigt att notera att detta är en enkel jämförelse och att andra faktorer (confounders) kan påverka resultatet.

\section*{Implementation-detaljer}

Koden använder följande tekniker:
\begin{itemize}
\item \textbf{NaN-hantering:} Alla NaN-värden filtreras bort med \texttt{$\sim$np.isnan()} innan analys.
\item \textbf{Kärnestimatorer:} Använder \texttt{stats.gaussian\_kde()} för att skapa smidiga fördelningskurvor.
\item \textbf{Visualisering:} Använder \texttt{plt.subplot()} för att skapa grid-layout med flera plotter.
\item \textbf{Dataextraktion:} Använder boolean indexing för att separera grupper (rökare/icke-rökare, drickare/icke-drickare).
\end{itemize}

\section*{Slutsatser}

Analysen bekräftar flera viktiga observationer:

\begin{enumerate}
\item \textbf{Födelsevikter:} Majoriteten av barnen har normala födelsevikter (3000-3500 gram), vilket är förväntat för en normal population.
\item \textbf{Moderns ålder:} De flesta mödrarna är i åldern 25-28 år, vilket är typiskt för första gångers mödrar.
\item \textbf{Rökningseffekt:} Det finns en tydlig och konsekvent skillnad mellan rökare och icke-rökare, där barn till rökare har signifikant lägre födelsevikter. Detta bekräftar att rökning är en viktig riskfaktor för låg födelsevikt.
\item \textbf{Medicinsk relevans:} Resultaten stödjer den medicinska definitionen av låg födelsevikt (under 2500 gram) som en viktig indikator för neonatal hälsa, och visar att rökning är en identifierbar och potentiellt förhindringsbar riskfaktor.
\end{enumerate}

Dessa resultat är i linje med tidigare forskning som visar att rökning under graviditeten är en av de viktigaste riskfaktorerna för låg födelsevikt och andra komplikationer vid födsel.

\chapter*{Analys av problem5.py: Test av normalitet}

Många statistiska metoder baseras på antagandet att data är normalfördelat. Det är därför viktigt att kunna avgöra om en given datamängd är normalfördelad eller ej. I detta kapitel använder vi två metoder för att testa normalitet: visuell bedömning med probplot (Q-Q plots) och statistiskt test med Jarque-Bera-testet.

\section*{Metod}

\subsection*{Visuell bedömning: Probplot}

Probplot (Probability-Quantile plot eller Q-Q plot) är en visuell metod där de observerade kvantilerna plottas mot teoretiska kvantiler från en normalfördelning. Om data är normalfördelat, kommer datapunkterna att ligga nära den röda referenslinjen. Avvikelser från linjen indikerar avvikelser från normalitet.

Funktionen \texttt{stats.probplot} används enligt:
\begin{lstlisting}[language=Python]
_ = stats.probplot(data, plot=plt)
\end{lstlisting}

\textbf{Viktigt:} Funktionen har en känd bugg där den röda referenslinjen inte visas om data innehåller NaN-värden. Därför filtreras NaN-värden bort innan anropet.

\subsection*{Statistiskt test: Jarque-Bera}

Jarque-Bera-testet är ett formellt statistiskt test baserat på datans skevhet (skewness) och kurtosis. Testvariabeln är:
\begin{equation}
JB = \frac{n}{6}\left(S^2 + \frac{1}{4}(K-3)^2\right)
\end{equation}
där $n$ är antalet observationer, $S$ är skattningen av skevhet (skewness) och $K$ är skattningen av kurtosis.

För en normalfördelad variabel $X$ med väntevärde $\mu$ och standardavvikelse $\sigma$ definieras:
\begin{align}
\gamma &= \E\left[\left(\frac{X-\mu}{\sigma}\right)^3\right] \quad \text{(skewness)} \\
\kappa &= \E\left[\left(\frac{X-\mu}{\sigma}\right)^4\right] \quad \text{(kurtosis)}
\end{align}

För en normalfördelning är $\gamma = 0$ och $\kappa = 3$ (eller excess kurtosis $\kappa - 3 = 0$).

Under nollhypotesen $H_0$: "Data är normalfördelat" är testvariabeln $JB$ approximativt $\chi^2$-fördelad med 2 frihetsgrader. Vi förkastar $H_0$ om $JB > \chi^2_{0.05}(2) \approx 5.9915$ eller om $p$-värdet $< 0.05$.

\section*{Resultat}

\subsection*{Barnets födelsevikt}

\textbf{Probplot:} Probplotet visar en S-formad avvikelse från den röda referenslinjen. I den nedre svansen (teoretiska kvantiler under -2) ligger datapunkterna under linjen, vilket indikerar färre mycket låga födelsevikter än förväntat för en normalfördelning. I den övre svansen (teoretiska kvantiler över 2) ligger datapunkterna något ovanför linjen, vilket tyder på fler mycket höga födelsevikter än förväntat. Den nedre svansens avvikelse är mer uttalad.

\textbf{Jarque-Bera test:}
\begin{itemize}
\item JB-statistik: 190.52
\item $p$-värde: $< 0.000001$ (praktiskt taget 0)
\item Beslut: \textbf{Förkasta $H_0$} - Data är \textbf{INTE} normalfördelat
\item Skewness ($\gamma$): -0.7250 (negativ skevhet, vänsterskewad)
\item Kurtosis ($\kappa$): 2.0047 (högre än 0, tyngre svansar än normalfördelning)
\end{itemize}

\textbf{Avvikelse:} Födelsevikten är vänsterskewad (negativ skewness) och har tyngre svansar än en normalfördelning (positiv excess kurtosis). Detta stämmer överens med probplotet som visar S-formad avvikelse.

\subsection*{Moderns ålder}

\textbf{Probplot:} Probplotet visar att datapunkterna följer den röda referenslinjen mycket nära över hela intervallet. Det finns inga signifikanta avvikelser i varken nedre eller övre svansen, vilket tyder på att fördelningen är nära normalfördelad.

\textbf{Jarque-Bera test:}
\begin{itemize}
\item JB-statistik: 18.54
\item $p$-värde: 0.000094
\item Beslut: \textbf{Förkasta $H_0$} - Data är \textbf{INTE} normalfördelat (men nära)
\item Skewness ($\gamma$): 0.3858 (lätt positiv skevhet, högerskewad)
\item Kurtosis ($\kappa$): -0.0151 (nära 0, mycket nära normalfördelning)
\end{itemize}

\textbf{Avvikelse:} Trots att probplotet visar en mycket god anpassning till normalitet, förkastar Jarque-Bera-testet nollhypotesen på 5\% signifikansnivå. Detta beror på den lilla men statistiskt signifikanta positiva skevheten (0.3858). I praktiken kan fördelningen betraktas som approximativt normalfördelad, men strikt sett avviker den från normalitet på grund av den lätt högerskewningen.

\subsection*{Moderns längd}

\textbf{Probplot:} Probplotet visar att datapunkterna följer den röda referenslinjen mycket nära, liknande moderns ålder. Det kan finnas en mycket lätt uppåtböjning i den övre extremen (teoretiska kvantiler över 2.5), men den är minimal.

\textbf{Jarque-Bera test:}
\begin{itemize}
\item JB-statistik: 4.83
\item $p$-värde: 0.089452
\item Beslut: \textbf{Acceptera $H_0$} - Data är normalfördelat
\item Skewness ($\gamma$): -0.1543 (mycket liten negativ skevhet)
\item Kurtosis ($\kappa$): 0.2520 (liten positiv excess kurtosis)
\end{itemize}

\textbf{Avvikelse:} Moderns längd kan betraktas som normalfördelad. Testet accepterar nollhypotesen på 5\% signifikansnivå ($p = 0.089 > 0.05$). Skewness och kurtosis är båda mycket nära 0, vilket bekräftar att fördelningen är nära normalfördelad.

\subsection*{Moderns vikt}

\textbf{Probplot:} Probplotet visar en tydlig avvikelse från den röda referenslinjen, särskilt i den övre svansen (teoretiska kvantiler över 1). Datapunkterna böjer sig betydligt uppåt, vilket indikerar att det finns fler höga viktvärden än förväntat för en normalfördelning. Den nedre svansen (teoretiska kvantiler under -2) visar också några punkter under linjen, men den övre svansens avvikelse är mycket mer uttalad.

\textbf{Jarque-Bera test:}
\begin{itemize}
\item JB-statistik: 279.40
\item $p$-värde: $< 0.000001$ (praktiskt taget 0)
\item Beslut: \textbf{Förkasta $H_0$} - Data är \textbf{INTE} normalfördelat
\item Skewness ($\gamma$): 0.9997 (stark positiv skevhet, högerskewad)
\item Kurtosis ($\kappa$): 2.2889 (högre än 0, tyngre svansar)
\end{itemize}

\textbf{Avvikelse:} Moderns vikt är starkt högerskewad (positiv skewness = 0.9997) och har tyngre svansar än en normalfördelning (positiv excess kurtosis = 2.2889). Detta stämmer väl överens med probplotet som visar en tydlig uppåtböjning i den övre svansen. Denna högerskewning är förväntad för viktfördelningar, eftersom det finns en naturlig nedre gräns men ingen strikt övre gräns.

\section*{Sammanfattning}

\begin{table}[ht]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Variabel} & \textbf{Probplot} & \textbf{JB-statistik} & \textbf{$p$-värde} & \textbf{Slutsats} \\
\midrule
Barnets födelsevikt & S-formad avvikelse & 190.52 & $< 0.000001$ & \textbf{Inte normalfördelad} \\
Moderns ålder & Mycket nära linjen & 18.54 & 0.000094 & \textbf{Nästan normalfördelad} \\
Moderns längd & Mycket nära linjen & 4.83 & 0.089452 & \textbf{Normalfördelad} \\
Moderns vikt & Tydlig högerskewning & 279.40 & $< 0.000001$ & \textbf{Inte normalfördelad} \\
\bottomrule
\end{tabular}
\caption{Sammanfattning av normalitetstester. Kritiskt värde för $\chi^2_2$ vid 5\% signifikansnivå: 5.9915.}
\end{table}

\subsection*{Implementation-detaljer}

Koden implementerar följande:
\begin{itemize}
\item \textbf{Probplot:} Använder \texttt{stats.probplot()} för att skapa Q-Q plots. Viktigt: NaN-värden måste filtreras bort innan anropet, annars visas inte den röda referenslinjen (känd bugg i scipy).
\item \textbf{Jarque-Bera test:} Använder \texttt{stats.jarque\_bera()} som returnerar både teststatistikan och $p$-värdet.
\item \textbf{Skevhet och kurtosis:} Beräknas med \texttt{stats.skew()} och \texttt{stats.kurtosis()} (excess kurtosis, där normal = 0).
\item \textbf{Kritiskt värde:} Beräknas med \texttt{stats.chi2.ppf(1 - ALPHA, df=2)} för $\chi^2$-fördelningen med 2 frihetsgrader.
\end{itemize}

\section*{Slutsatser}

\begin{enumerate}
\item \textbf{Moderns längd} är den enda variabeln som kan betraktas som normalfördelad enligt både visuell bedömning och statistiskt test.

\item \textbf{Moderns ålder} är nästan normalfördelad. Probplotet visar en mycket god anpassning, men testet förkastar nollhypotesen på grund av en liten men statistiskt signifikant positiv skevhet. I praktiken kan den betraktas som approximativt normalfördelad.

\item \textbf{Barnets födelsevikt} avviker från normalitet med en S-formad avvikelse i probplotet, vilket indikerar vänsterskewning och tyngre svansar. Detta bekräftas av testet med stark negativ skewness (-0.73) och positiv excess kurtosis (2.00).

\item \textbf{Moderns vikt} avviker starkt från normalitet med en tydlig högerskewning. Detta är förväntat för viktfördelningar och bekräftas av testet med stark positiv skewness (1.00) och positiv excess kurtosis (2.29).

\item \textbf{Praktisk relevans:} För variabler som inte är normalfördelade bör man vara försiktig med metoder som förutsätter normalitet (t.ex. t-test, ANOVA). Alternativa metoder som inte kräver normalitet (t.ex. icke-parametriska tester) kan vara mer lämpliga.
\end{enumerate}

\end{document}